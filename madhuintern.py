# -*- coding: utf-8 -*-
"""madhuintern.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z_ejV_XwtlaeyFYvBgrIMiJvD9Vc9EqN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

"""2. Load Dataset"""

df = pd.read_csv("card_transdata.csv")

df.head()

df.head(5)

"""3. Basic Summary Stats"""

df.describe()

"""4. Missing Values"""

df.isna().sum()

df.columns

df['fraud'].value_counts()
df['fraud'].value_counts(normalize=True)*100

sns.countplot(x='fraud', data=df)
plt.title("Fraud vs Non-Fraud Transactions")
plt.show()

"""5. Distribution of Key Variables"""

cont_cols= [
    'distance_from_home',
    'distance_from_last_transaction',
    'ratio_to_median_purchase_price'
]
for col in cont_cols:
  sns.histplot(df[col], bins=50,kde=True)
  plt.title(f"Distribution of {col}")
  plt.show()

df['distance_from_home'].describe()

"""6. Binning Columns- distance_from_home"""

min_dist = df['distance_from_home'].min()
max_dist = df['distance_from_home'].max()

bins = [
    min_dist, 5, 15, 20, 30, 40, 50, 75, 100, 200, 500, max_dist
]

df['distance_bin_custom'] = pd.cut(
    df['distance_from_home'],
    bins=bins,
    include_lowest=True
)

df['distance_bin_custom'].value_counts().sort_index()

"""7. Relationship With Target (fraud)"""

df.groupby('distance_bin_custom')['fraud'].agg(['count', 'mean'])

"""### Purpose of Analysis

To evaluate whether transactions occurring far from a cardholder’s home location exhibit higher fraud risk.

### Observations

- The feature shows a **right-skewed distribution**, indicating most transactions occur close to home, with a small number occurring at very large distances.
- Fraud rate remains relatively **low and stable** for short and medium distances.
- A **sharp increase in fraud probability** is observed when transaction distance exceeds a high threshold.

### Key Insight

> Transactions occurring unusually far from the cardholder’s home location are significantly more likely to be fraudulent, suggesting geographical deviation as a strong fraud indicator.
>

8) Binning Columns - distance_from_last_transaction
"""

df['distance_from_last_transaction'].describe()

bins = [0, 1, 5, 10, 20, 50, 100, np.inf]
labels = ['0–1', '1–5', '5–10', '10–20', '20–50', '50–100', '100+']

df['distance_from_last_transaction_bin'] = pd.cut(
    df['distance_from_last_transaction'],
    bins=bins,
    labels=labels,
    include_lowest=True
)

"""10.Relationship With Target (fraud)"""

df.groupby('distance_from_last_transaction_bin')['fraud'].agg(['count', 'mean'])

"""## Distance from Last Transaction (`distance_from_last_transaction`)

### Purpose of Analysis

To examine whether large gaps between consecutive transactions indicate suspicious activity.

### Observations

- Most transactions have a **small distance gap** from the previous transaction.
- Fraud rates remain stable (~8%) for short gaps.
- When the distance from the last transaction becomes **very large**, fraud probability increases dramatically (approaching ~50%).

### Key Insight

> Transactions that occur far away from the previous transaction location show a substantial increase in fraud risk, indicating abnormal transaction sequences.
>

11.Binning Columns- ratio_to_median_purchase_price
"""

df['ratio_to_median_purchase_price'].describe()

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df['ratio_to_median_purchase_price'], bins=50, kde=True)
plt.title("Distribution of Ratio to Median Purchase Price")
plt.show()

import numpy as np

bins_ratio = [0, 0.5, 1, 2, 5, np.inf]
labels_ratio = [
    "<0.5",
    "0.5–1",
    "1–2",
    "2–5",
    "5+"
]

df['ratio_to_median_purchase_price_bin'] = pd.cut(
    df['ratio_to_median_purchase_price'],
    bins=bins_ratio,
    labels=labels_ratio,
    include_lowest=True
)

df['ratio_to_median_purchase_price_bin'].value_counts().sort_index()

"""Relationship With Target (fraud)"""

df.groupby('ratio_to_median_purchase_price_bin')['fraud'].agg(['count', 'mean'])

"""## Ratio to Median Purchase Price (`ratio_to_median_purchase_price`)

### Purpose of Analysis

To determine whether deviations from a customer’s typical spending amount are associated with fraud.

### Observations

- The distribution is **highly right-skewed**, with most values clustered around 1 (normal spending behavior).
- Transactions within normal spending ranges (ratio < 2) show **low fraud rates (~2–3%)**.
- Fraud probability increases sharply for higher ratios:
    - Ratio 2–5 → ~13% fraud
    - Ratio >5 → **~62% fraud**

### Key Insight

> Transactions that significantly exceed a customer’s usual spending behavior are strongly associated with fraud.
>
"""

df['used_chip'].value_counts()

df.groupby('used_chip')['fraud'].agg(['count', 'mean'])

"""Transactions where chip authentication is used exhibit a significantly lower fraud rate compared to non-chip transactions, indicating chip-based payments as a strong fraud-reducing factor."""

df['used_pin_number'].value_counts()
df.groupby('used_pin_number')['fraud'].agg(['count', 'mean'])

"""PIN adds an extra authentication layer → should reduce fraud.

"""

df['online_order'].value_counts()
df.groupby('online_order')['fraud'].agg(['count', 'mean'])

"""Online transactions lack physical verification → higher fraud risk."""

df['repeat_retailer'].value_counts()
df.groupby('repeat_retailer')['fraud'].agg(['count', 'mean'])

"""Repeat Retailer vs Fraud — EDA Insight

Analysis of repeat retailer behavior shows minimal difference in fraud rates between repeat and non-repeat merchants. This suggests that merchant familiarity alone does not significantly influence fraud occurrence in the dataset.
"""

corr = df[
    [
        'distance_from_home',
        'distance_from_last_transaction',
        'ratio_to_median_purchase_price',
        'fraud'
    ]
].corr()

corr

"""Correlation Matrix"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""Correlation Analysis — EDA Insight

Correlation analysis reveals that the ratio to median purchase price has the strongest positive association with fraud, while distance-based features exhibit weaker linear relationships. Low inter-feature correlations indicate minimal redundancy, supporting the inclusion of all numerical variables for downstream modeling.

People who spend much more than usual are much more likely to commit fraud

Being far from home increases fraud risk a little

Sudden location jumps also increase risk a little

These three things are mostly independent of each other (they capture different behaviors)
"""

df.shape

"""Sweetviz Automatic Report"""

!pip install sweetviz

import sweetviz as sv
import pandas as pd

df['fraud'].isna().sum()

df = df.dropna(subset=['fraud'])

df['fraud'].isna().sum()

!pip install numpy==1.26.4

import pandas as pd
import sweetviz as sv

df = pd.read_csv("card_transdata.csv")
df = df.dropna(subset=['fraud'])

report = sv.analyze(df, target_feat='fraud')
report.show_html("sweetviz_fraud_report.html")

df.groupby(['online_order', pd.cut(df['distance_from_home'], bins=[0,10,50,100,500,df['distance_from_home'].max()])])['fraud'].mean()

"""For online vs offline transactions, check how fraud rate changes as distance from home increases."""

df.groupby(
    [
        'online_order',
        pd.cut(
            df['distance_from_last_transaction'],
            bins=[0, 1, 5, 10, 50, df['distance_from_last_transaction'].max()]
        )
    ]
)['fraud'].mean()

"""For online vs offline transactions, check how fraud rate changes as distance from last transaction increases."""

df.groupby(
    [
        'used_chip',
        pd.cut(
            df['distance_from_home'],
            bins=[0, 10, 50, 100, 500, df['distance_from_home'].max()]
        )
    ]
)['fraud'].mean()

"""chip usage reduces fraud even at higher distances from home"""

df.groupby(
    [
        'used_chip',
        pd.cut(
            df['distance_from_last_transaction'],
            bins=[0, 10, 50, 100, 500, df['distance_from_last_transaction'].max()]
        )
    ]
)['fraud'].mean()

"""Bivariate analysis between distance from last transaction and chip usage shows that fraud risk increases sharply for large transaction gaps when the card chip is not used. However, when chip-based authentication is present, the increase in fraud probability is significantly mitigated, highlighting the importance of security mechanisms in reducing fraud during abnormal transaction patterns."""

df.groupby(
    [
        'used_pin_number',
        pd.cut(
            df['ratio_to_median_purchase_price'],
            bins=[0, 0.5, 1, 2, 5, df['ratio_to_median_purchase_price'].max()]
        )
    ]
)['fraud'].mean()

"""analysis between PIN usage and transaction amount deviation shows that fraud probability increases sharply for high-value transactions when a PIN is not used. However, when PIN authentication is present, fraud rates remain extremely low even for unusually large purchases.

Define features & target
"""

X = df[
    [
        'distance_from_home',
        'distance_from_last_transaction',
        'ratio_to_median_purchase_price',
        'used_chip',
        'used_pin_number',
        'online_order',
        'repeat_retailer'
    ]
]

y = df['fraud']

X.isna().sum()

binary_cols = ['used_chip', 'used_pin_number', 'online_order', 'repeat_retailer']

for col in binary_cols:
    X[col].fillna(X[col].mode()[0], inplace=True)

X.isna().sum()

y.isna().sum()

y.fillna(y.mode()[0], inplace=True)

y.isna().sum()

"""split dataset into test and tran (70:30 ) ratio"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

"""using logistic regression

"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

"""for test dataset

"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

print("Accuracy:", accuracy_score(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_prob))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

y_train_pred = model.predict(X_train)
y_train_prob = model.predict_proba(X_train)[:, 1]

"""for the train dataset

"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

print("Train Accuracy:", accuracy_score(y_train, y_train_pred))
print("Train ROC-AUC:", roc_auc_score(y_train, y_train_prob))
print("Train Confusion Matrix:\n", confusion_matrix(y_train, y_train_pred))
print("Train Classification Report:\n", classification_report(y_train, y_train_pred))

"""Finding out the Important feature columns"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_scaled, y)

import pandas as pd
import numpy as np

feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lr.coef_[0],
    'Absolute_Coefficient': np.abs(lr.coef_[0])
})

feature_importance.sort_values(by='Absolute_Coefficient', ascending=False)

"""GridSearch for Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

"""Initialize Logistic Regression"""

log_reg = LogisticRegression(max_iter=1000)

"""Apply GridSearchCV"""

grid = GridSearchCV(
    estimator=log_reg,
    param_grid=param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)

"""Fit on TRAIN data only"""

grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best ROC-AUC:", grid.best_score_)

best_model = grid.best_estimator_

y_pred_tuned = best_model.predict(X_test)
y_prob_tuned = best_model.predict_proba(X_test)[:, 1]

from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report

print("Accuracy:", accuracy_score(y_test, y_pred_tuned))
print("ROC-AUC:", roc_auc_score(y_test, y_prob_tuned))
print(confusion_matrix(y_test, y_pred_tuned))
print(classification_report(y_test, y_pred_tuned))